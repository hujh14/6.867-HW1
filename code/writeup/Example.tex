\documentclass[a4paper,twoside]{article}

\usepackage{epsfig}
\usepackage{subfigure}
\usepackage{calc}
\usepackage{amssymb}
\usepackage{amstext}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{multicol}
\usepackage{pslatex}
\usepackage{apalike}
\usepackage{SCITEPRESS}
\usepackage[small]{caption}

\subfigtopskip=0pt
\subfigcapskip=0pt
\subfigbottomskip=0pt

\begin{document}

\title{6.867 Machine Learning  \subtitle{Homework 1} }

\maketitle

% **************************************************************************************************
 % Problem 1
% **************************************************************************************************

\section{\uppercase{Implementing Gradient Descent}}

\noindent Gradient descent is an iterative procedure for find a vector that minimize an objective function. Typically, the vector is our parameters and the objective function is the cost function. In each iteration, we find the gradient of the objective function evaluated at the vector and update the vector in the direction of the negative gradient.

\subsection{Basic Gradient Descent}

\noindent To demonstrate the gradient descent algorithm, we began by finding the minimum of two well defined functions with closed form derivatives shown below.

\medskip
\noindent Negative Gaussian:
\begin{equation}
f(x) = - \frac{1}{\sqrt{(2\pi)^n |\Sigma|}} exp[-\frac{1}{2} (x-u)^T\Sigma^{-1}(x-u)]
\end{equation}
\begin{equation}
\frac{\partial f(x)}{\partial x} = -f(x) \Sigma^{-1} (x-u)
\end{equation}

\noindent Quadratic Bowl:
\begin{equation}
f(x) = \frac{1}{2} x^T A x - x^T b
\end{equation}
\begin{equation}
\frac{\partial f(x)}{\partial x} = Ax - b
\end{equation}

How did we do?

The gradient descent algorithm takes three additional parameters, the starting guess, step size, and convergence criterion. Each affects the end result of the algorithm. The starting guess is important because gradient descent iteratively follows the gradient. Thus it can get stuck in local minimum. By running the algorithm repeated with random initialization, we increase our chance of finding a global minimum. The step size affects convergence behavior of the algorithm. As depicted in figure 1, if the step size is too small, the algorithm will converge slowly. If the step size is too big, the algorithm will overshoot the minimum and oscillate, converging more slowly. If the step size is much too big, the algorithm will actually diverge. This is depicted in figure 1 where for different step sizes, the norm of the gradients are plotted over time. The convergence criterion determines for how little change in cost function is it okay to stop. Decreasing this gives greater accuracy but increase runtime.

Include Graphs at the end!!

\subsection{Central Difference Approximation}

For many objective functions, it is impossible to write a closed form gradient function. Thus, use central difference approxiation to approximate the gradient. For each dimension $i$, estimate its partial derivative by 
\begin{equation}
\bigtriangledown_i f(x) = \frac{f(x+d*\hat{\i}) - f(x-d*\hat{\i})}{2d}
\end{equation}
for some small $d$. The larger $d$ is, the more inaccurate the gradient approximation is. We will use $d = 10^{-3}$.

\subsection{Batch vs. Stochastic Gradient Descent}
Now use gradient descent to find parameters $\theta$ that minimizes the least squared error objective function
\begin{equation}
J(\theta) = ||X\theta - y||^2
\end{equation}
where each row of $X$ and $y$ is a data sample pair. 

In batch gradient descent, $\theta$ is updated with the gradient of the cost function for the entire training dataset.
How quickly did we converge and what did we converge to? 

In contrast, stochastic gradient descent updates $\theta$ with the gradient of the cost function for each data point. $\theta$ is updated according to 
\begin{equation}
\theta_{t+1} = \theta_t - \eta_t \bigtriangledown_\theta J(\theta_t; x^{(i)}, y^{(i)}).
\end{equation}
 We iterate through the entire dataset $n$ times  We converge when the cost between iterations 


% **************************************************************************************************
 % Problem 2
% **************************************************************************************************

\section{\uppercase{Linear Basis Function Regression}}

You can write stuff here to but don't have to. In the supervised learning problem of regression, we are given a set of n Data points and n target values and the goal is to find a function that relates x to y. We have to do this in such a way that this function generalizes well to unseen values of x. Linear Basis Function Regression aims to find the optimal linear combination of basis functions to create a function mapping x to y. These basis functions take the form phi(x). To test our implementation, we generated 11 data points from the function y(x) = cos(pi*x)+1.5cos(2*pi*x) with some added noise. 

\subsection{Closed Form Solution}

From the textbook, we know the closed form solution for the maximum likelihood wieght vector given data values X, target values Y, and the value of M, where M is the highest order polynomial in our polynomial basis. The closed form solution for the maximum likelihood weight vector is (phi.T dot phi)**-1 dot phi.T dot Y. 

\subsection{Gradient Descent Objective Functions}

The objective function we were trying to minimize was Sum Squared Error. The function and its derivative are: . We then used our gradient descent algorithms to find the weight vector that minimized the Sum Squared error. 

Write functions to compute SSE and derivative. TODO LATER

\subsection{Batch vs Stochastic Gradient Descent}

We tested different combinations of convergence criterion, step size, initial guess, and M. We found that with batch gradient descent, a step size of 0.1 achieved the best results. The algorithm achieved lower costs on the cost function and required less iterations than smaller step sizes. Values lower than that would cause the algorithm to converge to a suboptimal value, and values higher than that caused the algorithm to oscillate and fail. We believe that this is because the loss function is less sensitive to changes, allowing for larger step sizes without oscillation. Additionally, we believe that this insensitivy to changes makes it easy for the function to have nearly identical values on successive iterations when the step size is small, causing the algorithm to converge to suboptimal weights. Lower convergence criterion led to better results, but came with the tradeoff of longer convergence times. For example, with M=4 and a step size of .01, lowering the convergence criterion from 10^-6 to 10^-7 quintupled the number of iterations needed and improved the cost by .02. We initialized our parameter vector at random and it didn't affect the converged values of the algorithm. 

With Stochastic Gradient Descent, we found similar trends. Lowering the convergence criterion led to lower costs but more iterations. Increasing the step size (with a max value of 1) generally led to lower costs. A step size of 1 would sometimes give the best results, but other times it caused the algorithm to oscillate and fail. We initialized our parameter vector at random several times and it didn't affect the converged values of the algorithm. 

Batch gradient descent achieved better results, yielding lower costs in nearly every configuration we tested. However, it took a significant number of more iterations than stochastic gradient descent. It often needed more than 10 times the number of iterations. 
Compare!

\subsection{Cosine Basis Functions}

We then tested our function for computing the maximum likelihood weight vector by using our prior knowledge about the origin of the data. We used cosine basis functions instead of polynomial ones. The real values of the function that generated the data are cos(pi*x)+1.5cos(2*pi*x), which means its weight vector would be [1, 1.5]. Our calculations yielded [0.7789928, 1.17413213]. 

Shtuff.

% **************************************************************************************************
 % Problem 3
% **************************************************************************************************

\section{\uppercase{Ridge Regression}}

Linear regressions, especially those with sum squared error cost functions, are prone to highly overfit the data. One way to ameliorate this is to introduce a regularization term on your weight vector. Ridge regression punishes weight vectors with large sizes. The parameter lambda controls how much the size of a vector is penalized. 

\subsection{Closed Form Solution}

We tested different values of lambda on ridge regressions on a small data set. The tested values of lamda ranging from 1000 to .00001. With 

\subsection{Results?}

% **************************************************************************************************
 % Problem 4
% **************************************************************************************************

\section{\uppercase{Sparsity and LASSO}}

\subsection{Lasso}



\vfill
\end{document}

